{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI - Assignment 3\n",
    "\n",
    "There are three questions that cover three chapters in RL study. \n",
    "\n",
    "Due date: March 29, 2021.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 (5 points)\n",
    "This question (Exercise 3.14 at page 60) is about the Bellman equation given at page 59 of the text book. \"The Bellman equation Eq.(3.14), at page 59, must hold for each state for the value function $v_{\\pi}$ shown in Figure 3.2 (right) of Example 3.5. Show numerically that this equation holds for the center state, valued at +0.7, with respect to its four neighboring states, valued at +2.3, +0.4, and 0.7\".\n",
    "\n",
    "This question asks you to verify the Bellman equation to check that at the center state the $v_{\\pi} \\approx 0.7$.\n",
    "\n",
    "In order to get the mark of this question, you must list the detail of Bellman's equation, not just Bellman's equation result, which is $v_{\\pi} \\approx 0.7$.\n",
    "\n",
    "Hint: use Bellman equation Eq.(3.14) to check the four actions (west, east, south, and north)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:\n",
    "    \n",
    "$$ \\pi(s, a= n) \\sum_{s'} P_{s, s'}^{a=n} (R_{s,s'}^{a=n} + \\gamma V^{\\pi}(s')) + \\\\\n",
    "    \\pi(s, a= e) \\sum_{s'} P_{s, s'}^{a=e} (R_{s,s'}^{a=e} + \\gamma V^{\\pi}(s')) + \\\\\n",
    "    \\pi(s, a= s) \\sum_{s'} P_{s, s'}^{a=s} (R_{s,s'}^{a=s} + \\gamma V^{\\pi}(s')) + \\\\\n",
    "    \\pi(s, a= w) \\sum_{s'} P_{s, s'}^{a=w} (R_{s,s'}^{a=w} + \\gamma V^{\\pi}(s'))  \\\\ \n",
    "    = \\frac{1}{4} (R_{s,s'}^{a=n} + \\gamma V^{\\pi}(s')) + \\frac{1}{4} (R_{s,s'}^{a=e} + \\gamma V^{\\pi}(s')) + \\\\\n",
    "    \\frac{1}{4} (R_{s,s'}^{a=s} + \\gamma V^{\\pi}(s')) + \\frac{1}{4} (R_{s,s'}^{a=w} + \\gamma V^{\\pi}(s'))\n",
    "    $$\n",
    "    \n",
    "Because $R=0$ for all states, above becomes:\n",
    "\n",
    "$$ \\frac{1}{4} ( \\gamma V^{\\pi}(s')) + \\frac{1}{4} ( \\gamma V^{\\pi}(s')) + \\\\\n",
    "    \\frac{1}{4} ( \\gamma V^{\\pi}(s')) + \\frac{1}{4} ( \\gamma V^{\\pi}(s')) \\\\\n",
    "    = \\frac{\\gamma}{4} (2.3 + 0.4 -0.4 +0.7) \\\\\n",
    "    = \\frac{0.9}{4} \\times 3.0 \\approx 0.7\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (5 points) \n",
    "\n",
    "This question is about the problem of policy prediction in dynamic programming algorithms. This is the only programming question. The question is given as:\n",
    "\n",
    "Refer to the text book page 76, the example 4.1, write a Python script to calculate the $v_k$ values in each state shown in the left column of the Figure 1 in page 77.\n",
    "\n",
    "Here are variables and initial values that you need for your script. You may need implement two functions:\n",
    "one for calculating action value based on given state; another is for policy evaluation (prediction). However, you are free to design your script. Following functions' definitation is only for your reference.\n",
    "\n",
    "The output of your script shall look like this:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "gamma = 1 # discounting rate\n",
    "rewardSize = -1\n",
    "gridSize = 4\n",
    "terminationStates = [[0,0], [gridSize-1, gridSize-1]]\n",
    "actions = ['left', 'right', 'up','down']\n",
    "numIterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "def actionValue(state, action):\n",
    "    if state in terminationStates:\n",
    "        return state, 0\n",
    "    reward = rewardSize\n",
    "    newState = np.array(state)\n",
    "    if action == 'left':\n",
    "        newState = newState + np.array([-1,0])\n",
    "    if action == 'right':\n",
    "        newState = newState + np.array([1,0])\n",
    "    if action == 'up':\n",
    "        newState = newState + np.array([0,1])\n",
    "    if action == 'down':\n",
    "        newState = newState + np.array([0, -1])\n",
    "    \n",
    "    if -1 in newState or 4 in newState:\n",
    "        newState = state\n",
    "    return newState, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: policy evaluation\n",
    "def policyEval():\n",
    "    deltas = []\n",
    "    pi_as = 1/len(actions)\n",
    "    valueMap = np.zeros((gridSize, gridSize))\n",
    "    states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "    for i in range(numIterations):\n",
    "        copyValueMap = np.copy(valueMap)\n",
    "        deltaState = []\n",
    "        for state in states:\n",
    "            weightedRewards = 0\n",
    "            for action in actions:\n",
    "                newState, reward = actionValue(state, action)\n",
    "                weightedRewards += pi_as * (reward + (gamma*valueMap[newState[0], newState[1]]))\n",
    "            deltaState.append(np.abs(copyValueMap[state[0], state[1]] - weightedRewards))\n",
    "            copyValueMap[state[0], state[1]] = weightedRewards\n",
    "        deltas.append(deltaState)\n",
    "        valueMap = copyValueMap\n",
    "        if i in [0,1,2,9, 99, numIterations-1]:\n",
    "            print(\"Iteration {}\".format(i+1))\n",
    "            print(valueMap)\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "\n",
      "Iteration 2\n",
      "[[ 0.   -1.75 -2.   -2.  ]\n",
      " [-1.75 -2.   -2.   -2.  ]\n",
      " [-2.   -2.   -2.   -1.75]\n",
      " [-2.   -2.   -1.75  0.  ]]\n",
      "\n",
      "Iteration 3\n",
      "[[ 0.     -2.4375 -2.9375 -3.    ]\n",
      " [-2.4375 -2.875  -3.     -2.9375]\n",
      " [-2.9375 -3.     -2.875  -2.4375]\n",
      " [-3.     -2.9375 -2.4375  0.    ]]\n",
      "\n",
      "Iteration 10\n",
      "[[ 0.         -6.13796997 -8.35235596 -8.96731567]\n",
      " [-6.13796997 -7.73739624 -8.42782593 -8.35235596]\n",
      " [-8.35235596 -8.42782593 -7.73739624 -6.13796997]\n",
      " [-8.96731567 -8.35235596 -6.13796997  0.        ]]\n",
      "\n",
      "Iteration 100\n",
      "[[  0.         -13.94260509 -19.91495107 -21.90482522]\n",
      " [-13.94260509 -17.92507693 -19.91551999 -19.91495107]\n",
      " [-19.91495107 -19.91551999 -17.92507693 -13.94260509]\n",
      " [-21.90482522 -19.91495107 -13.94260509   0.        ]]\n",
      "\n",
      "Iteration 1000\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policyEval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3. \n",
    "\n",
    "This question is about the Monte Carlo Methods on-policy and off-policy approaches. It is the Exercise 5.5 of the text book.\n",
    "\n",
    "\"Exercise 5.5 Consider an MDP with a single nonterminal state and a single action that transitions back to the nonterminal state with probability p and transitions to the terminal state with probability 1 - p. Let the reward be +1 on all transitions, and let $\\gamma=1$. Suppose you observe one episode that lasts 10 steps, with a return of 10. What are the first-visit and every-visit estimators of the value of the nonterminal state?\"\n",
    "\n",
    "Hint: the first-visit means run the first episode, and every-visit means after the 10 episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:\n",
    "    \n",
    "$$ G \\leftarrow \\gamma G + R_{t+1} $$\n",
    "\n",
    "$$ V(s) = \\frac{\\sum_{t \\in J(s)} \\rho_{t:T(t)-1} G_t}{|J(s)|} $$,\n",
    "\n",
    "while, in this case, $|J(s)|=10$.\n",
    "\n",
    "For the sirst-visit, $\\rho_{t:T(t)-1}=1$, then the first-visit $$V(s) = \\frac{10 + 10 + ... + 10}{10} = 10$$.\n",
    "\n",
    "For the every-visit case: since $G_0 = 0 $, $ \\gamma = 1 $, $R=+1$, So, we have:\n",
    "\n",
    "$$ G_1 = \\gamma G_0 + R_{0+1} = 1 $$,\n",
    "$$ G_2 = \\gamma G_1 + R_{1+1} = 2 $$,\n",
    "\n",
    "...\n",
    "\n",
    "$$ G_10 = \\gamma G_9 + R_{9+1} = 10 $$,\n",
    "\n",
    "So, we have:\n",
    "\n",
    "$$ V(s) = \\frac{(1+2+3+4+5+6+7+8+9+10)}{10} = 5.5 $$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
